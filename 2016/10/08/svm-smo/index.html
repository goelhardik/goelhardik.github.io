<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Implementing SVM SMO algorithm from scratch in Python &middot; Hardik Goel
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- fa-icon -->
  <script src="https://use.fontawesome.com/dc0101c58b.js"></script>
</head>


  <body class="layout-reverse theme-base-08">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
		<img src="/images/mugshot.png" height=150px width=auto style="margin-left: 50px; border-radius: 50%; border: 2px solid white; 
		    -ms-transform: rotate(1deg); /* IE 9 */
			    -webkit-transform: rotate(1deg); /* Chrome, Safari, Opera */
				    transform: rotate(1deg);" />
        <a href="/">
          Hardik Goel
        </a>
      </h1>
      <p class="lead">Computer Scientist & Graduate Student <h3 style="white-space: pre;"> <a href="https://github.com/goelhardik" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>    <a href="https://twitter.com/virtualbaba" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>    <a href="https://www.facebook.com/hardik.goel.549" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></h3></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About Me</a>
          
        
      
        
      
        
          
        
      

      <a class="sidebar-nav-item" href="https://github.com/goelhardik">Projects</a>
    </nav>

	<!--<p>&copy; 2016. All rights reserved.</p>-->
  </div>
</div>


    <div class="content container">
      <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<div class="post">
  <h1 class="post-title">Implementing SVM SMO algorithm from scratch in Python</h1>
  <span class="post-date">08 Oct 2016 &#9642;
	   <a href="https://goelhardik.github.io/2016/10/08/svm-smo/#disqus_thread">0 Comments</a></span>
  <div class="message">
    Support Vector Machines (SVM) are a type of models that can be used to perform classification or regression. They are effective because they can perform non-linear classification as well, using what is called the *kernel trick*. The optimization problem that is required to be solved for SVMs is a constrained Quadratic Programming (QP) problem, which can be a bit inefficient to solve if we have input features in high-dimensional spaces. To do this efficiently, an algorithm called Sequential Minimal Optimization (SMO) proposed by John Pratt <a href="#platt1998sequential">(Platt &amp; others, 1998)</a>, is used. We will try to implement this algorithm from scratch today in Python and evaluate it on a 2-class MNIST-13 dataset.
</div>

<p><strong>Short discussion on SVM classifiers</strong>
<br/>
<strong>Separable case</strong>
The SVM classifier is written as</p>

<p>$$f(w) = w^{\mathsf{T}}x + b$$</p>

<p>If the two classes are represented as \(y \in \{-1, +1\}\), classification is done by the sign of \(f(w)\). If \(f(w)\) is positive, then the example is classified as \(y = 1\), otherwise as \(y = 0\).
Maximizing the margins between the decision boundary and the closest data points, leads us to the following objective.</p>

<p>$$\min_{w, b} \frac{1}{2}\left\lVert w \right\rVert ^{2} \\
s.t.\space\space\space y_{i}(w^{\mathsf{T}}x + b) \ge 1 $$</p>

<p>This is the QP problem that can be solved using any QP software. However, if we transform this into a dual optimization problem, we will be able to use kernels that allow us to work in very high dimensional spaces. Once in that form, we will use the SMO algorithm that will be much more efficient than the QP software.
<br/>
The Lagrangian for our optimization problem above, can be constructed as</p>

<p>$$ \mathcal{L}(w, b, \alpha) = \frac{1}{2} \left\lVert w \right\rVert ^{2} - \sum_{i = 1}^{n} \alpha_{i}(y_{i}(w^{\mathsf{T}}x_{i} + b) - 1) $$</p>

<p>This is the Lagrange dual. Setting the gradient w.r.t. \(w\) and \(b\), we get</p>

<p>$$ w = \sum_{i = 1}^{n}\alpha_{i}y_{i}x_{i} $$
and,
$$ \sum_{i = 1}^{n}\alpha_{i}y_{i} = 0 $$</p>

<p>When we plug this back into the Lagrangian, we get our final dual optimization problem</p>

<p>$$ \mathcal{L}(\alpha) = \sum_{i = 1}^{n}\alpha_{i} - \frac{1}{2} \sum_{i, j = 1}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{\mathsf{T}}x_{j}^{\mathsf{T}} $$</p>

<p>which we need to maximize w.r.t. \(\alpha\), given the constraints</p>

<p>$$ \alpha_{i} \ge 0,  \forall i $$
and,
$$ \sum_{i = 1}^{n}\alpha_{i}y_{i} = 0 $$</p>

<p>If we find the solution \(d^{*}\) to this dual problem, it is the same as the primal solution \(p^{*}\) because the required conditions and the KKT conditions are satisfied as well. Thus once we find the optimal alphas, \(\alpha^{*}\), we can go back and find the optimal \(w^{*}\) and \(b^{*}\) as well.
<br/>
Note that no derivation is being shown here because our main topic is implementing the SMO algorithm, and discussion about Lagrangian derivations is a topic for another post.</p>

<p><strong>Non-separable case</strong>:
Note that in case if the data examples are not linearly separable, using similar though process a new Lagrangian dual objective can be derived, where everything remains the same, except the constraint on \(\alpha\)s now becomes</p>

<p>$$ 0 \le \alpha_{i} \le C, \forall i $$</p>

<p>where, \(C\) is the regularization parameter, which controls the relative weighting between the twin goals of making the \( \left\lVert w \right\rVert ^{2}\) small and of ensuring that most training set examples have functional margin of at least 1.
In this case, a reformulation of the dual problem can be written as below (in keeping with (missing reference).</p>

<p>$$ \min_{\alpha} \frac{1}{2}\alpha ^ {\mathsf{T}} Q \alpha - e^{\mathsf{T}} \alpha \\
s.t. \space\space\space y^{\mathsf{T}} \alpha = 0 \\
and \space\space\space 0 \le \alpha_{i} \le C $$</p>

<p><strong>Sequential Minimal Optimization (SMO)</strong>
<br/>
Note that the final problem formulated above is a Quadractic Programming problem and could have been solved using a QP library. However, the matrix \(Q\) sometimes may be a dense matrix and may be too large to be stored. So a decomposition method is considered to conquer this problem. A decomposition method is an iterative method where in each iteration only a subset of \(\alpha\) is modified, and thus only some columns of \(Q\) are needed. One extreme form of this decomposition is called Sequential Minimal Optimization (SMO). In SMO, only \(2\) \( \alpha\)s are selected and modified in each iteration.</p>

<p><strong>Implementation</strong>
<br/>
We will implement the SMO version presented in the paper <a href="#chang2011libsvm">(Chang &amp; Lin, 2011)</a>. The entire working code can be found <a href="">here</a>.
The important steps in the algorithm are as follows.
<ul>
<li>
Prepare some variables.
</li>
<li>
Perform Working Set Selection (WSS) to select two alphas, \(i\) and \(j\).
</li>
<li>
Update the working set alphas.
</li>
</ul></p>

<p>Each of the steps can be coded as below.
<ul>
<li>
<strong>Preparation :</strong>
<br/>
Calculate the Kernel and the Q matrix.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float&#39;</span><span class="p">)</span>                                                                                                                                                                                                             
<span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span></code></pre></figure>

<p>Calculate the quad coefficient for use in the WSS step. This does not change with iterations, so we can calculate and store it for later use to speed up our algorithm.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">a_ts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">):</span>
        <span class="n">a_ts</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">a_ts</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">a_ts</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tao</span>
        <span class="n">a_ts</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_ts</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">]</span></code></pre></figure>

<p></li></p>

<p><li>
<strong>WSS :</strong>
<br/>
This part refers to Section 4.1.2 in the Chang paper.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">wss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c"># calculate I_up and I_low</span>
    <span class="n">I_up</span> <span class="o">=</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">C_1</span><span class="p">))</span> <span class="o">|</span>
            <span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)))</span>
    <span class="n">I_low</span> <span class="o">=</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">C_minus_1</span><span class="p">))</span> <span class="o">|</span>
            <span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)))</span>
    <span class="c"># calculate gradient</span>
    <span class="n">del_f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alphas</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c"># select i</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">I_up</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">&#39;inf&#39;</span><span class="p">))</span>
    <span class="n">del_t_f_alpha_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">I_up</span><span class="p">,</span> <span class="n">del_f</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">&#39;inf&#39;</span><span class="p">))</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span> <span class="n">del_t_f_alpha_k</span><span class="p">))</span>
    <span class="c"># select j</span>
    <span class="n">minus_y_i_del_i_f_alpha_k</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">del_f</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">I_low</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">&#39;inf&#39;</span><span class="p">))</span>   
    <span class="n">del_t_f_alpha_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">I_low</span><span class="p">,</span> <span class="n">del_f</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s">&#39;inf&#39;</span><span class="p">))</span>   
    <span class="n">ind_minus_y_t_del_t_f_alpha_k</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span> <span class="n">del_t_f_alpha_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">minus_y_i_del_i_f_alpha_k</span>  
    <span class="n">j_ind</span> <span class="o">=</span> <span class="n">I_low</span> <span class="o">&amp;</span> <span class="n">ind_minus_y_t_del_t_f_alpha_k</span>   
    <span class="n">y_t_del_t_f_alpha_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">j_ind</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span> <span class="n">del_t_f_alpha_k</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s">&#39;inf&#39;</span><span class="p">))</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">j_ind</span><span class="p">,</span> 
                            <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">minus_y_i_del_i_f_alpha_k</span> <span class="o">+</span> <span class="n">y_t_del_t_f_alpha_k</span><span class="p">),</span> 
                                <span class="bp">self</span><span class="o">.</span><span class="n">a_ts</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]),</span> 
                            <span class="nb">float</span><span class="p">(</span><span class="s">&#39;inf&#39;</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span></code></pre></figure>

<p></li>
</ul></p>

<p><strong>What?</strong><br/>
As mentioned above, Fisher&rsquo;s LDA is a dimension reduction technique. Such techniques can primarily be used to reduce the dimensionality for high-dimensional data. People do this for multiple reasons - dimension reduction as feature extraction, dimension reduction for classification or for data visualizaiton.<br/></p>

<p><strong>How?</strong><br/>
Since this is the theory section, <strong>key takeaways</strong> from it are as follows (in case, you do not want to spend time on it)<br/>
1. Calculate \(S_{b}\), \(S_{w}\) and \(d^{\prime}\) largest eigenvalues of \(S_{w}^{-1}S_{b}\).<br/>
2. Can project to a maximum of \(K - 1\) dimensions.<br/>
<br/>
The core idea is to learn a set of parameters \(w \in \mathbb{R}^{d \times d^{\prime}}\), that are used to project the given data \(x \in \mathbb{R}^{d}\) to a smaller dimension \(d^{\prime}\). The figure below <a href="#bishop2006pattern">(Bishop, 2006)</a> shows an illustration. The original data is in 2 dimensions, \(d = 2\) and we want to project it to 1 dimension, \(d = 1\).
<table class="image">
    <caption align="bottom">LDA example</caption>
    <center><img src="/images/lda.png" alt="LDA example"/></center>
</table></p>

<p>If we project the 2-D data points onto a line (1-D), out of all such lines, our goal is to find the one which maximizes the distance between the means of the 2 classes, after projection. If we could do that, we could achieve a good separation between the classes in 1-D. This is illustrated in the figure on the left and can be captured in the idea of maximizing the &quot;<em>between class covariance</em>&quot;. However, as we can see that this causes a lot of overlap between the projected classes. We want to minimize this overlap as well. To handle this, Fisher&rsquo;s LDA tries to minimize the &quot;<em>within-class covariance</em>&quot; of each class. Minimizing this covariance leads us to the projection in the figure on the right hand side, which has minimal overlap. Formalizing this, we can represent the objective as follows.</p>

<p>$$J(w) = \frac{w^{\mathsf{T}}S_{b}w}{w^{\mathsf{T}}S_{w}w}$$</p>

<p>where \(S_{b} \in \mathbb{R}^{d \times d}\) and \(S_{w} \in \mathbb{R}^{d \times d}\) are the between-class and within-class covariance matrices, respectively. They are calculated as</p>

<p>$$S_{b} = \sum_{k = 1}^K (m_{k} - m)N_{k}(m_{k} - m)^{\mathsf{T}}$$</p>

<p>$$S_{w} = \sum_{k = 1}^K \sum_{n = 1}^{N_{k}} (X_{nk} - m_{k})(X_{nk} - m_{k})^{\mathsf{T}}$$</p>

<p>where \(X_{nk}\) is the \(n\)th data example in the \(k\)th class, \(N_{k}\) is the number of examples in class \(k\), \(m\) is the overall mean of the entire data and \(m_{k}\) is the mean of the \(k\)th class.
Now using Lagrangian dual and the KKT conditions, the problem of maximizing \(J\) can be transformed into the solution</p>

<p>$$S_{w}^{-1}S_{b}w = \lambda w$$</p>

<p>which is an eigenvalue problem for the matrix \(S_{w}^{-1}S_{b}\). Thus our final solution for \(w\) will be the eigenvectors of the above equation, corresponding to the largest eigenvalues. For reduction to \(d^{\prime}\) dimensions, we take the \(d^{\prime}\) largest eigenvalues as they will contain the most information. Also, note that if we have \(K\) classes, the maximum value of \(d^{\prime}\) can be \(K - 1\). That is, we cannot project \(K\) class data to a dimension greater than \(K - 1\). (Of course, \(d^{\prime}\) cannot be greater than the original data dimension \(d\)). This is because of the following reason. Note that the between-class scatter matrix, \(S_{b}\) was a sum of \(K\) matrices, each of which is of rank 1, being an outer product of two vectors. Also, because the overall mean and the individual class means are related, only (\(K - 1\)) of these \(K\) matrices are independent. Thus \(S_{b}\) has a maximum rank of \(K - 1\) and hence there are only \(K - 1\) non-zero eigenvalues. Thus we are unable to project the data to more than \(K - 1\) dimensions.
<br/></p>

<p><strong>Code</strong>
<br/>
The main part of the code is shown below. If you are looking for the entire code with data preprocessing, train-test split etc., find it <a href="https://github.com/goelhardik/projects/tree/master/fishers_lda">here</a>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c"># Function estimates the LDA parameters</span>
    <span class="k">def</span> <span class="nf">estimate_params</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="c"># group data by label column</span>
        <span class="n">grouped</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span><span class="bp">self</span><span class="o">.</span><span class="n">labelcol</span><span class="p">])</span>

        <span class="c"># calculate means for each class</span>
        <span class="n">means</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">:</span>
            <span class="n">means</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_col</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classwise</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">labelcol</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span>

        <span class="c"># calculate the overall mean of all the data</span>
        <span class="n">overall_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_col</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">labelcol</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span>

        <span class="c"># calculate between class covariance matrix</span>
        <span class="c"># S_B = \sigma{N_i (m_i - m) (m_i - m).T}</span>
        <span class="n">S_B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">means</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">S_B</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classwise</span><span class="p">[</span><span class="n">c</span><span class="p">]),</span>
                               <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">((</span><span class="n">means</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">-</span> <span class="n">overall_mean</span><span class="p">),</span> 
                                        <span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">-</span> <span class="n">overall_mean</span><span class="p">)))</span>

        <span class="c"># calculate within class covariance matrix</span>
        <span class="c"># S_W = \sigma{S_i}</span>
        <span class="c"># S_i = \sigma{(x - m_i) (x - m_i).T}</span>
        <span class="n">S_W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">S_B</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">:</span> 
            <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_col</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classwise</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">labelcol</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">S_W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="n">tmp</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">S_W</span><span class="p">)</span>

        <span class="c"># objective : find eigenvalue, eigenvector pairs for inv(S_W).S_B</span>
        <span class="n">mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">S_W</span><span class="p">),</span> <span class="n">S_B</span><span class="p">)</span>
        <span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">mat</span><span class="p">)</span>
        <span class="n">eiglist</span> <span class="o">=</span> <span class="p">[(</span><span class="n">eigvals</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">eigvecs</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eigvals</span><span class="p">))]</span>

        <span class="c"># sort the eigvals in decreasing order</span>
        <span class="n">eiglist</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">eiglist</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reverse</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

        <span class="c"># take the first num_dims eigvectors</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">eiglist</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_dims</span><span class="p">)])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">w</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">means</span> <span class="o">=</span> <span class="n">means</span>
        <span class="k">return</span>

    <span class="c"># estimate the LDA parameters</span>
    <span class="n">estimate_params</span><span class="p">(</span><span class="n">traindata</span><span class="p">)</span></code></pre></figure>

<p>The code is pretty self-explanatory if you followed the theory above and read the comments in the code. Once we estimate the parameters, there are two ways to classify it.<br/>
<ul>
<li>
<strong>Thresholding</strong><br/>
In one-dimensional projections, we find a threshold \(w_{0}\), which can be basically the mean of the projected means in the case of 2-class classification. Points above this threshold go into one class, while the ones below go in to the other class.
Here is the code to do that.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">Function to calculate the classification threshold.</span>
<span class="sd">Projects the means of the classes and takes their mean as the threshold.</span>
<span class="sd">Also specifies whether values greater than the threshold fall into class 1 </span>
<span class="sd">or class 2.</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">calculate_threshold</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c"># project the means and take their mean</span>
    <span class="n">tot</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">tot</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w0</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tot</span>

    <span class="c"># for 2 classes case; mark if class 1 is &gt;= w0 or &lt; w0</span>
    <span class="n">c1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="o">.</span><span class="n">keys</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">c2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="o">.</span><span class="n">keys</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">[</span><span class="n">c1</span><span class="p">])</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">mu1</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c1</span> <span class="o">=</span> <span class="s">&#39;ge&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c1</span> <span class="o">=</span> <span class="s">&#39;l&#39;</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">Function to calculate the scores in thresholding method.</span>
<span class="sd">Assigns predictions based on the calculated threshold.</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">calculate_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_col</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">labelcol</span><span class="p">)</span>
    <span class="c"># project the inputs</span>
    <span class="n">proj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="c"># assign the predicted class</span>
    <span class="n">c1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="o">.</span><span class="n">keys</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">c2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="o">.</span><span class="n">keys</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c1</span> <span class="o">==</span> <span class="s">&#39;ge&#39;</span><span class="p">):</span>
        <span class="n">proj</span> <span class="o">=</span> <span class="p">[</span><span class="n">c1</span> <span class="k">if</span> <span class="n">proj</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w0</span> <span class="k">else</span> <span class="n">c2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">proj</span><span class="p">))]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">proj</span> <span class="o">=</span> <span class="p">[</span><span class="n">c1</span> <span class="k">if</span> <span class="n">proj</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">w0</span> <span class="k">else</span> <span class="n">c2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">proj</span><span class="p">))]</span>
    <span class="c"># calculate the number of errors made</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">proj</span> <span class="o">!=</span> <span class="n">data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">labelcol</span><span class="p">])</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span></code></pre></figure>

<p></li>
<li></p>

<p><strong>Gaussian Modeling</strong><br/>
In this method, we project the data points into the \(d^{\prime}\) dimension, and then model a multi-variate Gaussian distribution for each class&rsquo; likelihood distribution \(P(x | C_{k})\). This is done by calculating the means and covariances of the data point projections. The priors \(P(C_{k})\) are estimated as well using the given data by calculating \(\frac{N_{k}}{N}\) for each class \(k\). Using these, we can calculate the posterior \(P(C_{k}|x)\) and then the data points can be classified to the class with the highest probability value. These ideas are solidified in the code below.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">Function to estimate gaussian models for each class.</span>
<span class="sd">Estimates priors, means and covariances for each class.</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">gaussian_modeling</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">priors</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gaussian_means</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gaussian_cov</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_col</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classwise</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">labelcol</span><span class="p">)</span>
        <span class="n">proj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">priors</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gaussian_means</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">proj</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gaussian_cov</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">proj</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">Utility function to return the probability density for a gaussian, given an </span>
<span class="sd">input point, gaussian mean and covariance.</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">point</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">):</span>
    <span class="n">cons</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">point</span><span class="p">)</span><span class="o">/</span><span class="mf">2.</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">cons</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="n">point</span><span class="o">-</span><span class="n">mean</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">cov</span><span class="p">)),(</span><span class="n">point</span><span class="o">-</span><span class="n">mean</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="mf">2.</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">Function to calculate error rates based on gaussian modeling.</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">calculate_score_gaussian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_col</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">labelcol</span><span class="p">)</span>
    <span class="c"># project the inputs</span>
    <span class="n">proj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="c"># calculate the likelihoods for each class based on the gaussian models</span>
    <span class="n">likelihoods</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">priors</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> 
                                                        <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))],</span> <span class="bp">self</span><span class="o">.</span><span class="n">gaussian_means</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> 
                           <span class="bp">self</span><span class="o">.</span><span class="n">gaussian_cov</span><span class="p">[</span><span class="n">c</span><span class="p">])</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> 
                    <span class="n">classes</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">proj</span><span class="p">])</span>
    <span class="c"># assign prediction labels based on the highest probability</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">likelihoods</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">labels</span> <span class="o">!=</span> <span class="n">data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">labelcol</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">errors</span></code></pre></figure>

<p></li>
<strong>Experiments</strong><br/>
I used two datasets - Boston housing data and the Digits data. 
<ul>
<li>
<a href="https://archive.ics.uci.edu/ml/datasets/Housing">Boston data</a> has 13 dimensional data points with continuous valued targets, however one can convert the data into categorical data for classification experiments as well. In this case, I converted it into 2 class data, based on the median value of the target. For this dataset, I explore the threshold classification method.
After using the above code for estimating the parameters and a threshold for classification, I evaluate it on the test set, which gives an error rate of ~14%. The 1-D data projections can be plotted to visualize the separation better. I have used the code below.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">plot_proj_1D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)))</span>
    <span class="n">plotlabels</span> <span class="o">=</span> <span class="p">{</span><span class="n">classes</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="p">:</span> <span class="n">colors</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))}</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">proj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">row</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">labelcol</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">proj</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span>
                    <span class="n">plotlabels</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">labelcol</span><span class="p">]])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p>I have added some random noise to the y-value of the projections so that the points do not overlap, making it difficult to see.
Plotting the projections for the entire data, the figure looks something like this.
<table class="image">
    <caption align="bottom">LDA : 1-D Projections for Boston data with added y-noise</caption>
    <center><img src="/images/lda.boston.jpg" alt="LDA : 1-D Projections for Boston data with added y-noise"/></center>
</table></p>

<p></li>
<li>
<a href="https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits">Digits</a> is a dataset of handwritten digits 0 - 9. It has 64 dimensional data with 10 classes. I used this data as it was for classification. For this dataset, I perform the projection of data into 2 dimensions and then use bivariate Gaussian modeling for classification. After evaluation, the error rate comes out to be ~30% on the test set, which is not bad considering the 64-dimesional, 10-class data is projected to 2-D. For the 2-dimensional scatter plot for the data projections looks like this.</p>

<table class="image">
    <caption align="bottom">LDA : 2-D Projections for Digits data</caption>
    <center><img src="/images/digits.2d.jpg" alt="LDA : 2-D Projections for Digits data"/></center>
</table>

<p>Though there is overlap in the data in 2-D, some classes are well separated as well. The overlap is expected due to the very-low dimensional projection. I think given this constraint, LDA does a good job at projection.
I thought it would be fun to plot the likelihood gaussian curves as well. So here is the code to do that and the plot obtained.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">plot_bivariate_gaussians</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>                                                                                                                                                    
    <span class="n">classes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)))</span>
    <span class="n">plotlabels</span> <span class="o">=</span> <span class="p">{</span><span class="n">classes</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="p">:</span> <span class="n">colors</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))}</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">ax3D</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">&#39;3d&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gaussian_means</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> 
                                             <span class="bp">self</span><span class="o">.</span><span class="n">gaussian_cov</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">pdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">cons</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mf">2.</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gaussian_cov</span><span class="p">[</span><span class="n">c</span><span class="p">])</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">))</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="n">point</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">cons</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="n">point</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">gaussian_means</span><span class="p">[</span><span class="n">c</span><span class="p">]),</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gaussian_cov</span><span class="p">[</span><span class="n">c</span><span class="p">])),(</span><span class="n">point</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">gaussian_means</span><span class="p">[</span><span class="n">c</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="mf">2.</span><span class="p">)</span>

        <span class="n">zs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">pdf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ponit</span><span class="p">))</span> <span class="k">for</span> <span class="n">ponit</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> 
                                                               <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">Y</span><span class="p">))])</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">zs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">surf</span> <span class="o">=</span> <span class="n">ax3D</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                   <span class="n">color</span><span class="o">=</span><span class="n">plotlabels</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
                                   <span class="n">antialiased</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<table class="image">
    <caption align="bottom">LDA : Projected data likelihood Gaussian plots for Digits data</caption>
    <center><img src="/images/digits.gaussian.jpg" alt="LDA : Projected data likelihood Gaussian plots for Digits data"/></center>
</table>

<p></li>
</ul>
Hope this was fun and helpful for you to implement your own version of Fisher&rsquo;s LDA.
If you would like to run the code and produce the results for yourself, follow the <a href="https://github.com/goelhardik/projects/tree/master/fishers_lda">github</a> link to find the runnable code along with the two datasets - Boston and Digits.</p>

<p><strong>References:</strong>
<ol class="bibliography"><li><span id="platt1998sequential">Platt, J., &amp; others. (1998). Sequential minimal optimization: A fast algorithm for training support vector machines.</span></li>
<li><span id="chang2011libsvm">Chang, C.-C., &amp; Lin, C.-J. (2011). LIBSVM: a library for support vector machines. <i>ACM Transactions on Intelligent Systems and Technology (TIST)</i>, <i>2</i>(3), 27.</span></li>
<li><span id="bishop2006pattern">Bishop, C. M. (2006). Pattern recognition. <i>Machine Learning</i>, <i>128</i>.</span></li></ol></p>

</div>

<div id="disqus_thread"></div>
<script>

/**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */
/*
   var disqus_config = function () {
       this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
	       this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
		   };
		   */
(function() { // DON'T EDIT BELOW THIS LINE
     var d = document, s = d.createElement('script');
	     s.src = '//http-goelhardik-github-io.disqus.com/embed.js';
		     s.setAttribute('data-timestamp', +new Date());
			     (d.head || d.body).appendChild(s);
				 })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/10/04/fishers-lda/">
            Implementing Fisher&rsquo;s LDA from scratch in Python
            <small>04 Oct 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/06/04/mnist-autoencoder/">
            Building autoencoders in Lasagne
            <small>04 Jun 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/05/25/sampling-sine-wave/">
            Generating and visualizing data from a sine wave in Python
            <small>25 May 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

	<script id="dsq-count-scr" src="//http-goelhardik-github-io.disqus.com/count.js" async></script>
  </body>
</html>
