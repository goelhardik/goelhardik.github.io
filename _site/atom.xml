<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Hardik Goel</title>
 <link href="http://github.com/goelhardik/atom.xml" rel="self"/>
 <link href="http://github.com/goelhardik/"/>
 <updated>2016-05-27T01:37:09-05:00</updated>
 <id>http://github.com/goelhardik</id>
 <author>
   <name>Mark Otto</name>
   <email></email>
 </author>

 
 <entry>
   <title>Generating and visualizing data from a sine wave in Python</title>
   <link href="http://github.com/goelhardik/2016/05/25/sampling-sine-wave/"/>
   <updated>2016-05-25T00:00:00-05:00</updated>
   <id>http://github.com/goelhardik/2016/05/25/sampling-sine-wave</id>
   <content type="html">&lt;div class=&quot;message&quot;&gt;
    This blog post shows how to generate sequential data by sampling a sine wave (or any other kind of mathematical function). I did this in Python and found a cool way to visualize the data as it is generated.
&lt;/div&gt;

&lt;p&gt;So the objective here is to generate sequential data from a mathematical function - a &lt;em&gt;sine wave&lt;/em&gt; is used in this post. Also, the goal is to use Python to do this. We are going to sample a sine wave at a pre-defined interval and dump it to a file for future use in other Python scripts.&lt;/p&gt;

&lt;p&gt;Starting with the imports:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pylab&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pickle&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We will use these modules to get our work done.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;matplotlib.pyplot&lt;/strong&gt; to plot and visualize the data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;numpy&lt;/strong&gt; to generate the mathematical function&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pylab&lt;/strong&gt; to help with interactive plots&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pickle&lt;/strong&gt; to dump the data into a file for future use&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next, we set our range for the &lt;em&gt;x-axis&lt;/em&gt; and define our functions.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# define functions&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This will generate two lists &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; with our &lt;em&gt;x-axis&lt;/em&gt; and &lt;em&gt;y-axis&lt;/em&gt; data.&lt;/p&gt;

&lt;p&gt;We&amp;#39;ll now dump the &lt;em&gt;y-axis&lt;/em&gt; data into our file for future use.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# write the data out to a file&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sinedata&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;sinedata.md&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sinedata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sinedata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The above snippet writes the data of &lt;code&gt;y&lt;/code&gt; into a file named &lt;em&gt;sinedata.md&lt;/em&gt;. Pickle is specific to Python and it can be used to load the data into another Python script later.&lt;/p&gt;

&lt;p&gt;Next, we will visualize the data. Here is the code to do this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# interactive mode on&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pylab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# set the data limits&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# plot the first 200 points in the data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# plot the remaining data incrementally&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0005&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;span class=&quot;c&quot;&gt;# hold the plot until terminated&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I think the comments in the snippet do a good job at the explanation.
The plot will look something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/optimised.gif&quot; alt=&quot;placeholder&quot; title=&quot;Sine wave data&quot;&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Learning to predict a mathematical function using LSTM</title>
   <link href="http://github.com/goelhardik/2016/05/25/lstm-sine-wave/"/>
   <updated>2016-05-25T00:00:00-05:00</updated>
   <id>http://github.com/goelhardik/2016/05/25/lstm-sine-wave</id>
   <content type="html">&lt;div class=&quot;message&quot;&gt;
    &lt;strong&gt;Long Short-Term Memory (LSTM)&lt;/strong&gt; is an RNN architecture that is used to learn time-series data over long intervals. Read more about it &lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;here&lt;/a&gt;.
&lt;br /&gt;
    In this blog post, I&#39;ll share how I used an LSTM model to learn a &lt;i&gt;sine wave&lt;/i&gt; over time and then how I used this model to generate a sine-wave on its own.
&lt;/div&gt;

&lt;p&gt;In my &lt;a href=&quot;/2016/05/25/sampling-sine-wave/&quot;&gt;previous post&lt;/a&gt;, I shared how I used Python to generate sequential and periodic data from a sine wave. I dumped this data into a file called &lt;em&gt;sinedata.md&lt;/em&gt; last time, and we are going to use that dump in this post.&lt;/p&gt;

&lt;p&gt;For the LSTM, I have used the library called &lt;a href=&quot;https://github.com/Lasagne&quot;&gt;Lasagne&lt;/a&gt;. It is a great library for easily setting up deep learning models. Also, they provide some &lt;a href=&quot;https://github.com/Lasagne/Recipes&quot;&gt;&amp;quot;Recipes&amp;quot;&lt;/a&gt; for quick setup. I have used the LSTM model they provided for text generation and modified it to suit my needs for learning a sine-wave. So I will only share the relevant code in this post to avoid the clutter.&lt;/p&gt;

&lt;p&gt;First we&amp;#39;ll use &lt;em&gt;pickle&lt;/em&gt; to load the data that was generated earlier:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;in_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;sinedata.md&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;in_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The parameters used for the LSTM are as below:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# Sequence Length&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SEQ_LENGTH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Number of units in the two hidden (LSTM) layers&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N_HIDDEN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Optimization learning rate&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;LEARNING_RATE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;01&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# All gradients above this will be clipped&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;GRAD_CLIP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# How often should we check the output?&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;PRINT_FREQ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Number of epochs to train the net&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;NUM_EPOCHS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Batch Size&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;BATCH_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The LSTM architecture contains just 1 hidden layer with a &lt;code&gt;tanh&lt;/code&gt; non-linearity. The output layer has &lt;code&gt;linear&lt;/code&gt; non-linearity.
There is only 1 output unit. So given the last 50 sine wave samples at a distance of 0.1 &lt;em&gt;x-units&lt;/em&gt; each, our network will learn to predict the 51st point. Then given the last 49 samples from the data and the generated sample as the 50th sample, our network will predict the 51st sample once again. It will keep doing this, moving forward in time, for ~200 time steps in our case.&lt;/p&gt;

&lt;p&gt;So, for this experiment, I have generated sine-wave data for &lt;code&gt;x&lt;/code&gt; ranging from 0 to 2000 at a gap of 0.1. I train the LSTM on this data.&lt;/p&gt;

&lt;p&gt;The gif below shows what the network predicted after each training iteration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/sine-wave-prediction.gif&quot; alt=&quot;placeholder&quot; title=&quot;Sine wave predicted&quot;&gt;&lt;/p&gt;

&lt;p&gt;Some things to note:&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The network does not take much time to train; probably because of the &lt;em&gt;sequence length&lt;/em&gt; of 50 and 32 &lt;em&gt;hidden units&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The prediction is almost perfect&lt;/li&gt;
&lt;li&gt;The network can be trained using continuous data and also to predict continuous data&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let us track the variation of training time and number of required epochs with change in sequence length. The target training error is &lt;code&gt;&amp;lt; 0.0001&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Time Taken&lt;/em&gt; vs &lt;em&gt;Sequence Length&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/time-vs-seq.png&quot; alt=&quot;placeholder&quot; title=&quot;TrainingTime vs SeqLen&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Number of Epochs&lt;/em&gt; vs &lt;em&gt;Sequence Length&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/epocs-vs-seq.png&quot; alt=&quot;placeholder&quot; title=&quot;Epochs vs SeqLen&quot;&gt;&lt;/p&gt;

&lt;p&gt;Some things to note:&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Sequence length of 20 seems to be enough for training error of the order of 0.0001 &lt;/li&gt;
&lt;li&gt;Time taken increases as sequence length goes beyond 20, which is expected because of the increased complexity of the model &lt;/li&gt;
&lt;li&gt;Number of epochs remains the same more or less, for sequence length beyond 20&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let us now track the variation of training time and number of required epochs with change in the number of units in the hidden layer of the LSTM. The target training error is again &lt;code&gt;&amp;lt; 0.0001&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Time Taken&lt;/em&gt; vs &lt;em&gt;# Hidden Units&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/times-vs-nhid.png&quot; alt=&quot;placeholder&quot; title=&quot;TrainingTime vs #Hidden&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Number of Epochs&lt;/em&gt; vs &lt;em&gt;# Hidden Units&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/epochs-vs-nhid.png&quot; alt=&quot;placeholder&quot; title=&quot;Epochs vs #Hidden&quot;&gt;&lt;/p&gt;

&lt;p&gt;Some things to note:&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;~15 seems to be a reasonable number of hidden units for training error of the order of 0.0001 &lt;/li&gt;
&lt;li&gt;Time taken increases as number of hidden untis goes beyond 15, which is expected because of the increased complexity of the model &lt;/li&gt;
&lt;li&gt;Number of epochs remains the same more or less &lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;A possible explanation for these observations is that the sine-wave is pretty easy to learn. If the network knows the last ~20 values, it can predict what the next value should be. This optimal sequence length should be higher for more complex functions. Also the number of hidden units of ~15, seems to be good for learning to model a sine-wave.&lt;/p&gt;
</content>
 </entry>
 

</feed>
